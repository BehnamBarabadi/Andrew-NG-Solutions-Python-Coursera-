{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML_HW 2: Logistic Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exercise, you will implement logistic regression and apply it to two different datasets. \n",
    "\n",
    "Before we begin with the exercises, we need to import all libraries required for this programming exercise. Throughout the course, we will be using [`numpy`](http://www.numpy.org/) for all arrays and matrix operations, and [`matplotlib`](https://matplotlib.org/) for plotting. In this assignment, we will also use [`scipy`](https://docs.scipy.org/doc/scipy/reference/), which contains scientific and numerical computation functions and tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optimization module in scipy\n",
    "import scipy.optimize as opt\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Logistic Regression (40pt)\n",
    "\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and\n",
    "you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions\n",
    "decision. Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams. \n",
    "\n",
    "The following cell will load the data and corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "      <th>Admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Exam 1     Exam 2  Admitted\n",
       "0  34.623660  78.024693         0\n",
       "1  30.286711  43.894998         0\n",
       "2  35.847409  72.902198         0\n",
       "3  60.182599  86.308552         1\n",
       "4  79.032736  75.344376         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "# The first two columns contains the exam scores and the third column\n",
    "# contains the label.\n",
    "path = r'C:\\Users\\Behnam\\Downloads\\Python\\Datasets\\ML_HW2\\ex2data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Exam 1', 'Exam 2', 'Admitted'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualizing the data (10pt)\n",
    "\n",
    "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. Write codes below to display a figure where the axes are the two exam scores, and the positive and negative examples are shown with different markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "admitted = data.loc[data.Admitted == 1]\n",
    "# filter out the Admitted Stuents \n",
    "\n",
    "not_admitted = data.loc[data.Admitted == 0]\n",
    "# filter out the not Admitted Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(num=None, figsize=(10, 6), dpi=60,)\n",
    "plt.scatter(admitted['Exam 1'], admitted['Exam 2'], s=10, label='Admitted', linewidth=5, color = 'magenta')\n",
    "plt.scatter(not_admitted['Exam 1'], not_admitted['Exam 2'], s=10, label='Not Admitted', linewidth=5, color ='lawngreen') \n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Exam 1', color='aqua',fontsize=20)\n",
    "plt.ylabel('Exam 2', color='aqua',fontsize=20)\n",
    "plt.title('Data', color='aqua',fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lmplot('Exam 1', 'Exam 2', data, hue='Admitted', fit_reg=False)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "### 1.2 Implementation (30pt)\n",
    "\n",
    "#### 1.2.1 Warmup exercise: sigmoid function (5pt)\n",
    "\n",
    "Before you start with the actual cost function, recall that the logistic regression hypothesis is defined as:\n",
    "\n",
    "$$ h_\\theta(x) = g(\\theta^T x)$$\n",
    "\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as: \n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$.\n",
    "\n",
    "Your first step is to implement this function `sigmoid` so it can be\n",
    "called by the rest of your program. When you are finished, try testing a few\n",
    "values by calling `sigmoid(x)` in a new cell. For large positive values of `x`, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating `sigmoid(0)` should give you exactly 0.5. Your code should also work with vectors and matrices. **For a matrix, your function should perform the sigmoid function on every element.**\n",
    "<a id=\"sigmoid\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        The input to the sigmoid function. This can be a 1-D vector \n",
    "        or a 2-D matrix. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # convert input to a numpy array\n",
    "    z = np.array(z)\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    g = np.zeros(z.shape)\n",
    "    # g.shape is (m,)\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "   \n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell evaluates the sigmoid function at `z=0`. You should get a value of 0.5. You can also try different values for `z` to experiment with the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g( (-10, -3, 0, 1, 500) ) =  [0.0000454  0.04742587 0.5        0.73105858 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Test the implementation of sigmoid function here\n",
    "z = (-10, -3, 0, 1, 500)\n",
    "g = sigmoid(z)\n",
    "\n",
    "print('g(', z, ') = ', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to plot the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAGZCAYAAACDs1dwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de1xUdf7H8TcgXlMxUUkgtcKyu79CM9eyzBTbovaWWopWam3uprYtaZnb2pa2u5mbZubdyti2TNnWUkvLbuqY4CUlQSnB+73ykoLn98e3YUBmmEFmODPM6/l4fB9z+86ZD4eDvvnyPecbIckSAAAAEGYi7S4AAAAAsANBGAAAAGGJIAwAAICwRBAGAABAWCIIAwAAICwRhAEAABCWCMJAuBszRrIs02680e5qgtuNN7r21ZgxVd+ec1vLl1d9W/CP115zfV/i4+2uBkCAEYSBUFE6hJ1ty8+3+6tAIFT2OJgwwe6Kq89NN5lfWsaMkRIT7a4GQJCpZXcBAAAEzM03S08+ae5/+KFUUGBvPQCCCkEYCBUbN0p33lm595x7rjRlilSnjnn8zjvl+zz9tGnw7pNPpIgIu6uomC/HyNatga8jVPXrZxqAsEAQBkLFgQPSwoW+94+IkBYtcoXgTz6R0tMDUxuCR2WOEQAIc8wRBmqqv/5V6tnT3N+xQ/rd76TiYntrAgAgiBCEgZro9tulUaPM/ZMnpd/+Vtq7133fylw1omtX6a23pMJC6fhxaft2af58qXt387ovV1U480oJjRtLTzwhZWVJhw+bke/PP5f69i0/DeGKK6Tp06UtW6Rjx6Q9e8x0j//7P592i6KipPvvl/73P/PLwYkT0v79ksMhjR0rxcVV/P7KXDXi/POlf/1Lys111bpihTR4sKkjmHTr5vq6nniian2jolyvL11qnqtfX/rTn6Q1a6RDh6Qff5Q2bJCeecZ8/311113mqg65udL330s//WS+j0uWSI8/XvZkuLFjTQ3O+cGS9Nln5U8cdNboVJmrRlx6qTRxopm2dPiw+T5/+62UkSHdcYf3r6egwHxObq55HBUlDRli6ty3z2xvyxbzGeed59MuAlB5Fo1Gq0HtoossHTpkybJMe/jhivuPGePqe+ONnvu98IKrn7s2YYJ5v/PxmDHut+N8fflyS+3aWdq61fM2Z8xwvW/IEEsnT7rvd/KkpdTUir/OpCRLmzdX/DX88IOlfv08b8OXr08ytfzwg+fPWbHCUuPGZfdFVb7npbd9Nu/v1s31/ieeqFrfqCjX60uXWrrwQkubNnneF1u3WkpM9H5Mr11b8ffOsixt2eJ6z9ix3vs7ayz9Wa+95notPt5zTWPHWjp1quJtf/ihpZgYz9soKDD9cnMtNWtm6fPPPW9r3z5LV11Vff+O0Ghh0pgjDNQk9etL774rxcSYx6+9Jk2eXPXtPvWUNHy4uV9UZEa8PvrIjKhefrkZZR02TGrZ0vdtNm5s5rO2amW2t3SpGSn8v/+THn5YOucc6b77pE8/NaN/r7wi7d4tzZxpRhPr1pV+8xvpttuk6Ghp1iypbVszwnum+Hgzyta8uXmcmyvNni3l5UlNmpjRu169zGfOnm2mkMybd3b76vrrzah57drm8WefSf/+txmRb91aSkuTunQxX0dNFxNj5qlfdJE5Lj/4wIwIX3ih9NBDZtT8ggvMPu/Wzf02Lr5Y+uILc+KnZEaAMzKk9evNiGmLFtK110q//GXZvyC88YYZge7b10wLkqSRI6XNm8tu39NfSiry/PPSY4+Z+0VF0ptvmr9wHD8uXXml+Xlo3tx8TcuWSdddZ/4y40l0tPnLyvXXm5+rBQvMsZ6QID3wgHTZZVJsrPm6r7jCfCYAv7E9jdNoND+1N990jSBlZ1uqV8/7e7yNCF98saWffjKvHz1qqWvX8n2aNLH01VdlR7C8jQhblqVjx8wI45l9One2VFxs+mzbZkbDvvjCjKKe2Xf6dNf2HnvM/WcuWuTq89ZblmrXLt8nLc1SUZHpc+SIpbi48n28jQhHRpYddX766fJ9oqMtZWSU3Q81dUTYsiwdP26pZ8/y/Zo2tZSf7+rXvr37bW3c6Ooza5alunXd11WrlqXbbiv/fOmR4c6dve8LbyPCpY/N77+3dP317r+20j8Pzz7r/rOcI8KWZbZ5333l+9Sta2n1ale/u+6q2rFCo9HObLYXQKPR/NGGD3f9Z3nwoKULLvDtfd6C8EsvuV7/8589bycpyRWYfQ3C6emet/fBB2XDVEKC+37x8a5g8uGH5V+/4grXdrZt8xykzvxax44t/7q3IJya6np92TLPn1OvnqVvvw1MEPbWZs0q//5ABuHHH/e8rSFDKu7Xr1/ZfRQRUfl94+8gvHCh6/UHHvC8ndatzS96zl+sGjYs36d0EH7lFc/b6tHDt340Gq3SjZPlgJrghhuk8ePN/dOnpXvvlbZt88+2U1PN7YkTZnqCJ7m50vvv+77doqKKt/f55677//2vOUHPnR07zAlKkjl56Uy/+pXr/ksvma/Dk+efN/vvzPf56q67XPf/+U/P/Y4f98+UlWB36pT08sueX1+2zHXf3ffunntc9594QrIs/9V2NurWdV2JZc8eMx3Hk2+/NVNiJKlRI+mWWyre9sSJnl/75BPXFV/c7ScAZ405wkCoa9nSzEmNjjaPx4418zL9oXlz15n4WVlmrm5FPv7YFZy9+eYb6cgRz6/v2eO6v3p1xdvas8fMNW3SpPxrHTq47i9ZUvF2CgqknBwTNi65RGrYUPrhh4rfU1pysrktLnZdFcOTjz7yfbuV4W1Bje3bA/O57mzeXPExs2OH6767713nzuZ2/34zT9hu7du75n4vX+79coRLlkgDBpj7HTuaedLuHDlSfu5yaSdOSAcPSs2aud9PAM4aQRgIZdHR0ttvmxOGJBOA/blKXOmT33wZYa7MKPSBAxW//tNPle9bt27510pfdsp5maqKbNlignBkpLmcWmWCsHN/7d5tTuSqSF6e79utjGBaUMPdiYullf4en/m9i4kxJy9K5peTYFD6WNqyxXv/0n0quvyZt+NbqvgYB3DWmBoBhLKJE6VOncz9bdvMn5L9+efjBg1c970FO0k6etT3bTunIPi775kaNjS3p05VfOa+048/ln+vr5zBzd/7KlRV5fvWqJHrfunviZ1KHw++fP98PZaqsp8AVAlBGAhVaWnmElSSCV6//rW5qL8/lf7Pvn597/1LB+dg4RzRjY52TR+piDPMln6vr5zBJ1T3lS8iq+m/jdJTKkp/T+xU+njw5ftXlWMJQLUgCAOhqH17acoU1+OHHpKys/3/OTt3uu5fcIH3/r70qW67drnuJyV57+/sc/q0meJQGc79FRfnPQxfdFHlth1IpacoOOfAehIbG9hanA4fdoXHSy6pns/05myPJanszxKAoEEQBkJNkyZmWeF69czjl1+W5s4NzGft3WtOIJNM+C7952p3unYNTB1VUfpEO+dS0J4kJLhCV05O5UfxnJ8VFeV9X3haQMIOpf+S4G1RlI4dA1tLac4rh8TGmsUmzkbpaQdnLtldWVlZruk1N93kfXT81ltd972d8AnAFgRhIJRERJhVrNq0MY9XrjQrugWS8+SrunWlBx/03C8pSUpJCWwtZ2P+fNf9P/xBqlPHc9/HHjMhVjK/bFRW6asCjBjhuV/duq5pLcFgy5ayAc+TJk3Mpfmqy+uvu+7/7W9nF2RLz9Ot6nSUEydclwiMi5P69/fc9/zzpbvvNve//96snAgg6BCEgVAydqzUo4e5v2ePWWL41KnAfuakSa6QNGaM+5HOJk1MQPf2Z3U7bNjgupzchReaa7+6myt8771maWfJBJeKrn/ryXvvua5w0K2bNHp0+T61akkzZrh+mQkGJ0+aa9VKZh+5+4WnQQOzxG/TptVXV0aGtHGjud+1q1mW2tNVE6Ki3P8ilp/vuv9//1f1mv7+d9co84svmuWTz9Skibmai3N6zOTJwXPCH4AyuHwaECpuv10aOdL1eN486dprTauMzz7z7XJNTt98Y0bjnn7a/Me+dKkJvcuWmRGyyy+X7r/fjJC99Zb0u9+Z9wXTmfCDB0tr15rrIvfpYwLRnDnmEmYxMdIdd0i//KWr/0MPVX5+sGS+5vvvN9eYrV1b+utfzXSMf//bTDNp1cpcV/ayy8xI9dks2hEo//iHa+rIpEnmGr6LF5tftC6/XBo4UIqPN9/7Pn2qp6biYvPL3hdfSOeea/Zd9+4mIK9bZxYmadbMfD9vv91MZTlz7u6KFeZriI6W0tPNqPKGDa5f7vbvl776yveaPv/c7Ks//1lq3Nhsf948cw1t58/DAw+4LmmYlSX95S9+2BkAAsX25e1oNJoPbdasyi2l66mduYyytyWWnW3ChIq3O2FC2eV3hw1zvx1flxVOS3P1TUuruO/y5a6+nvokJVnavLnir+HHH82yvp624W2JZWe7805LP/zg+XM++cRS48aBWWK5Ktt55hnPNRcVmSWVK7PE8tKlFX+er33btrW0YYP3Y/ubb9y/f/x4z+8583O9LbFcel+dOlVxPR99ZCkmxvM2nEss5+Z6/95Upi+NRvO5MTUCgG+GDzfzR99+25wB/9NPZtnjd9810zWGDy/7Z/ODB+2r1Z3cXOmKK8xo3QcfmCsAnDxp6vzqKzPqnZQkvfZa1T9rwQIz6vvSS9LWrWakcN8+Mxr/4IPSzTdXvKqeXZ580iwh/L//mXp/+smcLPnmm1KXLmYf2WHLFunKK81I9NtvS999Z0aDncfg4sXSn/7k+QTF9HSpb1/zfd+927frSXvz5JPSVVeZ7/GmTWY6zYkTZuW+//zHrPDXrZv/L2kIwK8iZBIxAFTdP/4hPfqoud++fWAu6QYAgJ8QhAH4R6NGZtS1eXMzmnjeeWaOJwAAQYqpEQC8a9Gi4gUEGjc2fw5u3tw8njmTEAwACHqMCAPwrnNnc3b8qlXmahFbtpjllxs3Nmfs9+ljzuqXzJzYq6/mclEAgKDH5dMA+CYyUurUyTRP1q83l7EiBAMAQgAjwgC8q1PHLFbQs6dZYrdZM3OFiNOnzXzgNWvMdXEzMoLr+sEAAFTAtiC8d+9efffdd3Z8NAAAAMJIq1at1Nx5Hksptk2N+O6775ScnGzXxwMAACBMOBwOt89z1QgAAACEJYIwAAAAwhJBGAAAAGGJIAwAAICwRBAGAABAWCIIAwAAICwRhAEAABCWCMIAAAAISwRhAAAAhCWvQXjGjBnas2ePNmzY4LHPxIkTlZubq3Xr1ql9+/Z+LRAAAAAIBK9BePbs2erZs6fH11NSUpSUlKSkpCQNHjxYU6ZM8WuBAAAAQCDU8tbh008/VatWrTy+npqaqrlz50qSVq1apZiYGMXFxWn37t3+qxIAAMCPIiKkqCgpKlKKjDSPIyN+vo0sdasznjuzz8+Pz6aPsw5vtxHyva9UcX/nfXf7o9xzvvbz8TlJ+ni1dOIn96/ZwWsQ9iY+Pl4FBQUljwsLCxUfH+82CA8aNEiDBw+WJMXGxlb1owEAQJCpU1uqX09qUE+qX1dqUP/n23pl75fuU7+eFF1LqhUl1XLeRpmg6rxfy1+v/3w/krOkbJF4k1QYRGOlVQ7CEW4iv2VZbvtOmzZN06ZNkyQ5HI6qfjQAAKiiOrWl+BZSQpzU/Nzy4fXMUFty381r9eua8FkZp05Jx05IJ09JRcVSUZFUfNrcFhW7nit9v/i09NPJil/39n7nc6dPS6ctc2s5b+V6bFnmdat0nzMen20fZ1pyxqaKbkvuV+I9nvp7iGlun3f7XBXeu/eA+8+2S5WDcGFhoRITE0seJyQkaOfOnVXdLAAAqKJ6dU3ITYwzQTehRdnbxDip2bkVb+P4CRNUjx77+fa4dOy4dOh7acde1+Ojx8v3O/M97voUFVXPvgDcqXIQzszM1NChQ5WRkaGOHTvqyJEjzA8GACDAGtR3H2xLP24aU/59+w+ZP00X7pFWrze3hbulgt1mtO7HUiH1+AkzegnUVF6D8Lx589S1a1fFxsaqoKBAY8aMUXR0tCRp6tSpWrRokXr16qW8vDwdO3ZMAwcODHjRAADUZA0buA+2pQNvTKPy79t7wATb73ZKn611BV7n7Y49JtwCMCLkfqpHwDkcDiUnJ9vx0QAABJWmMdKtnaVeN0jdr5dauDmffNe+8sHWOZJbuFvaudfMmwVQnqfcWeWpEQAAoHIiIqRrLpNSbpBSukgdrzRXMdh7QFr8ubQup1Tg3S3t3GdOKgPgXwRhAACqwbkx0q3Xm/Db8xdS86Zm/u3qDdLTk6VFK6SvvvZ8Rj8A/yMIAwAQABERUvt2Jvj2usGM+kZFmZPVFn9mgu+Sz81jAPYgCAMA4Ccxjcxc35QuZtQ3rpl5fvV66ZlXTPhds5ErMQDBgiAMAEAVXN3OjPimdJE6XW1GfQ8eNnN9F60wo7/7DtpdJQB3CMIAAFRC44bmyg4pXcy0h/N+HvVds1F69lUTflevZ9QXCAUEYQAAvLjy4p9HfW+Qrr9aqlVLOnREWvKFtOgTM/q7Z7/dVQKoLIIwAABniI6Wbu/qurxZfAvz/NpN0vjpZtR31XqpuNjWMgFUEUEYAIBSLr1IeuN5M/f38PfS0i9M8P3gM2n3PrurA+BPBGEAAGQud/bHftK4EdL3P0q/eURauEwqKrK7MgCBQhAGAIS9+BbS7OekWzpJmcukB0ZzpQcgHBCEAQBh7e5e0pSnpNrR0qCnpOn/sbsiANWFIAwACEuNG0qTR0v33C6tXCfd+2dp63a7qwJQnQjCAICw07WDNOc5qWVz6amXpGencgUIIBwRhAEAYaN2tPTMI9KjA6W87dL1fSXHBrurAmAXgjAAICxcniS9/rx01SXSlAzpT89Lx47bXRUAOxGEAQA1WkSENKy/9NwIc13g2x40q8EBAEEYAFBjJcSZucA3Xyct+NBcFWL/IburAhAsCMIAgBqpz23Sy09JtaKk+5+UZr5jd0UAgg1BGABQo8Q0MgG4z23SF1lSv3RpW4HdVQEIRgRhAECNcVNHMxXivGbSkxOlcdO4LBoAzwjCAICQV6e29OxwacQAKWeb1KmPtGaj3VUBCHYEYQBASLuirfTG383t5HnSY3+Xjp+wuyoAoYAgDAAISZGRZgT4mUekg0eklMHSB5/aXRWAUEIQBgCEnPNbmrnAXTtI85dKg5+SDhy2uyoAoYYgDAAIKffcLk0ebUaEB46SZr9rd0UAQhVBGAAQEpo0lqaMke5OkT77Sur/uJRfaHdVAEIZQRgAEPS6dZJmPyu1aCqNmiCNny6dPm13VQBCXaTdBQAA4EndOtKEkdKHM6UfjkrX9Zaee5UQDMA/GBEGAASl6Gjp4zlSx6ukl16X0v/JZdEA+BdBGAAQlMb83oTg3o9K/15kdzUAaiKmRgAAgs51V0uPD5JmvEMIBhA4BGEAQFCpX89cI7hgtzT8OburAVCTMTUCABBUxj8qtW0t3ZRmTpADgEBhRBgAEDRuuV4aeo80YY708Wq7qwFQ0xGEAQBBoXFDadbfpM1bzbWCASDQmBoBAAgK/3pCiouV7vqDdOInu6sBEA4YEQYA2O6u7lL/VOlvU6U1G+2uBkC4IAgDAGzVvKk09S/SV19Lz7xidzUAwglBGABgq1eflho2kPqlS0VFdlcDIJwwRxgAYJu0O6XUbtKIceYkOQCoTowIAwBscX5Lc4LcJw7pxbl2VwMgHBGEAQDVLiLCXCotIkIaMFKyLLsrAhCOmBoBAKh2Q++Rbr5OemC09O0Ou6sBEK4YEQYAVKuL25hllN/7WJrxtt3VAAhnBGEAQLWJipLmjpOOnZAGPWV3NQDCHVMjAADVZuRgqcOV0m+HSbv32V0NgHDHiDAAoFq0v1R66iFp3nvS24vtrgYACMIAgGpQp7b02nhp70Fp6DN2VwMABlMjAAABN/aP0mUXST0HSYeO2F0NABg+jQj36NFDOTk5ys3NVXp6ernXGzVqpMzMTGVnZ2vjxo0aMGCAv+sEAISoLtdKjw6UpmRIiz+zuxoAKMuqqEVGRlp5eXlWmzZtrOjoaCs7O9tq165dmT4jR460xo0bZ0myYmNjrQMHDljR0dEVbtfhcFT4Oo1Go9FCv51TX9bWJbLyFstqUN/+emg0Wng2T7nT64hwhw4dlJeXp/z8fJ06dUoZGRlKTU0t08eyLDVs2FCSdM455+jgwYMqKirytmkAQA33z3SpdbyUNlI6eszuagCgLK9BOD4+XgUFBSWPCwsLFR8fX6bPpEmT1K5dO+3cuVMbNmzQI488Iov1MgEgrKXcIA3+nfT3mdLna+2uBgDK8xqEIyIiyj13Zsjt0aOHsrOz1bJlS1199dWaNGlSyQhxaYMGDZLD4ZDD4VBsbGwVygYABLNzY6QZz0gbtkhP/cvuagDAPa9BuLCwUImJiSWPExIStHPnzjJ9Bg4cqPnz50uStm7dqvz8fF1yySXltjVt2jQlJycrOTlZ+/fvr2rtAIAgNXm01LSx1C9dOnnK7moAwD2vQdjhcCgpKUmtW7dWdHS0evfurczMzDJ9tm/frm7dukmSmjdvrosvvljbtm0LTMUAgKB2dy+pdy/p6ZeldTl2VwMAnnm9jnBxcbGGDh2qxYsXKyoqSjNnztSmTZs0ZMgQSdLUqVM1duxYzZ49W+vXr1dERITS09N14MCBgBcPAAgu5zWTXh4trVwnjZ9udzUAULEImctHVDuHw6Hk5GQ7PhoAECCLpko3JktX/0rK/dbuagDA8JQ7WVkOAOAXg35rrhQxdCwhGEBo8GllOQAAKnJBovRCuvThl9LLb9pdDQD4hiAMAKiSyEhp9rNSUbE0cJTEZeQBhAqmRgAAqmTEAKnLtVL/x6XC3XZXAwC+Y0QYAHDWLkuSnnlEmr9Uem2h3dUAQOUQhAEAZyU6WnptnHTkB2nIGLurAYDKY2oEAOCsjH5Ian+plPqwtP+Q3dUAQOUxIgwAqLQOV0qjBkuz35Uyl9ldDQCcHYIwAKBS6tWV5o6TduyRHnnW7moA4OwxNQIAUCnjRkgXt5FuHiB9/6Pd1QDA2WNEGADgs5uvk/7YT5o4V1q+yu5qAKBqCMIAAJ80Okea9Tfpm3xp5AS7qwGAqmNqBADAJxNHSfEtpOv7SsdP2F0NAFQdI8IAAK9Su0kD7pKefVVavd7uagDAPwjCAIAKxTaRXn1aWrtJGjvF7moAwH+YGgEAqNCw/iYMdxsonTpldzUA4D+MCAMAPKpXV3qwt7TgI2ljrt3VAIB/EYQBAB71T5WaxkgvzLa7EgDwP4IwAMCtiAhpeJo5Oe7ztXZXAwD+xxxhAIBbvW40K8j1edTuSgAgMBgRBgC4NSJNKtglvb3E7koAIDAIwgCAcq66xCyn/K/XpaIiu6sBgMAgCAMAyhmeJv14VJr2H7srAYDAIQgDAMqIayb16SXNnC8d+cHuagAgcAjCAIAyHu4r1aolTXzN7koAILAIwgCAEvXqSg/9vIDGtgK7qwGAwCIIAwBKOBfQmDDH7koAIPAIwgAASa4FNBwbpM++srsaAAg8FtQAAEiSUm5gAQ0A4YURYQCAJBbQABB+CMIAAF11idStk/TSGyygASB8EIQBABrWnwU0AIQfgjAAhLm4ZlLf28wCGoe/t7saAKg+BGEACHMsoAEgXBGEASCMORfQWLiMBTQAhB+CMACEsX53mAU0XphtdyUAUP0IwgAQplhAA0C4Y0ENAAhTKTdIl1wg9f2T3ZUAgD0YEQaAMDU8TSrcLf1nsd2VAIA9CMIAEIauvFi6pZP0r9dZQANA+CIIA0AYGp7GAhoAQBAGgDDjXEBj1rssoAEgvBGEASDM/L4PC2gAgEQQBoCwUnoBja3b7a4GAOxFEAaAMNLvDim2iTRhjt2VAID9CMIAECYiIqRh/aU1G6VP19hdDQDYjwU1ACBM9OwitbuQBTQAwIkRYQAIEyMGsIAGAJRGEAaAMOBcQOOlN1hAAwCcCMIAEAaG9ZeOHpNefcvuSgAgeBCEAaCGi2sm3fNLaeZ8FtAAgNJ8CsI9evRQTk6OcnNzlZ6e7rbPjTfeqKysLG3cuFEff/yxP2sEAFQBC2gAgGdWRS0yMtLKy8uz2rRpY0VHR1vZ2dlWu3btyvRp3Lix9fXXX1uJiYmWJKtZs2YVblOS5XA4vPah0Wg0WtVa3Tqy9n0h692X7K+FRqPR7GqecqfXEeEOHTooLy9P+fn5OnXqlDIyMpSamlqmT9++fTV//nwVFBRIkvbt2+dtswCAauBcQOMFFtAAgHK8BuH4+PiSgCtJhYWFio+PL9Onbdu2atKkiZYvX641a9aoX79+brc1aNAgORwOORwOxcbGVrF0AEBFIiKk4WksoAEAnnhdUCMiIqLcc5Zlld1IrVq65ppr1K1bN9WrV09ffvmlVq5cqdzc3DL9pk2bpmnTpkmSHA5HVeoGAHjhXEDjnsfsrgQAgpPXIFxYWKjExMSSxwkJCdq5c2e5Pvv379exY8d07NgxrVixQldddVW5IAwAqD7D01hAAwAq4nVqhMPhUFJSklq3bq3o6Gj17t1bmZmZZfosXLhQXbp0UVRUlOrVq6eOHTtq8+bNASsaAFCxK9pK3a83C2icOmV3NQAQnLyOCBcXF2vo0KFavHixoqKiNHPmTG3atElDhgyRJE2dOlU5OTn64IMPtH79ep0+fVrTp0/X119/HfDiAQDuDU9jAQ0A8CZC5vIR1c7hcCg5OdmOjwaAGq1FrPTdR9K0/0h/eMbuagDAfp5yJyvLAUAN8/s+UjQLaACAVwRhAKhB6taRHuotZS6T8r6zuxoACG4EYQCoQe69Q2p2LgtoAIAvCMIAUENEREjD+0tffc0CGgDgC69XjQAAhIYev5AuvYgFNADAV4wIA0ANMWIAC2gAQGUQhAGgBrg8ySygMYkFNADAZwRhAKgBShbQ+I/dlQBA6CAIA0CIaxEr3XO7NOtd6dARu6sBgNBBEAaAEPdQbxbQAICzQRAGgBBWt45ZSe6/y1lAAwAqiyAMACGMBTQA4OwRhAEghDkX0FjhsNd4bhsAABv5SURBVLsSAAg9LKgBACGKBTQAoGoYEQaAEDVigLRjDwtoAMDZIggDQAi6PEm6tbP00ussoAEAZ4sgDAAhaBgLaABAlRGEASDENG8q3Xu7NHsBC2gAQFUQhAEgxPy+DwtoAIA/EIQBIISUXkAj91u7qwGA0EYQBoAQcs/tLKABAP5CEAaAEDIiTVq7iQU0AMAfCMIAECKcC2i8MNvuSgCgZiAIA0CIGJ5mFtB46wO7KwGAmoEgDAAh4LIkMyI86Q0W0AAAfyEIA0AIGP7zAhpT37K7EgCoOQjCABDkWEADAAKDIAwAQe6h3iygAQCBQBAGgCDmXEDjvY9ZQAMA/I0gDABB7J7bzdQILpkGAP5HEAaAIDb85wU0PmEBDQDwO4IwAASpWztLl10kTWA5ZQAICIIwAASpEQOknXulf79vdyUAUDMRhAEgCDkX0HjpdRbQAIBAIQgDQBAa1p8FNAAg0AjCABBknAtozFnIAhoAEEgEYQAIMg/1NtcPfnGu3ZUAQM1GEAaAIFKntllAI3MZC2gAQKARhAEgiDgX0OCSaQAQeARhAAgiIwZIWZukj1fbXQkA1HwEYQAIEs4FNF5gNBgAqgVBGACCxPA0FtAAgOpEEAaAIHBZktSzizTpDRbQAIDqQhAGgCAwrL907DgLaABAdSIIA4DNmp1rFtCYvUA6eNjuagAgfBCEAcBmzgU0JrKABgBUK4IwANioTm3p4b7Sf5dLW761uxoACC8EYQCwkXMBjRdm210JAIQfgjAA2Gh4mpS9mQU0AMAOBGEAsEn366XLk1hAAwDs4lMQ7tGjh3JycpSbm6v09HSP/a699loVFRXp17/+td8KBICaasQAs4BGxiK7KwGA8OQ1CEdGRmry5MlKSUnRpZdeqj59+qhdu3Zu+40fP16LFy8OSKEAUJNcehELaACA3bwG4Q4dOigvL0/5+fk6deqUMjIylJqaWq7fH/7wB73zzjvau3dvQAoFgJqEBTQAwH5eg3B8fLwKCgpKHhcWFio+Pr5Mn5YtW+quu+7SK6+8UuG2Bg0aJIfDIYfDodjY2LMsGQBCW7NzpX53SHMWsoAGANjJaxCOiIgo95xlWWUev/jii0pPT9fp06cr3Na0adOUnJys5ORk7d+/v5KlAkDN4FxA40VOkgMAW9Xy1qGwsFCJiYkljxMSErRz584yfa699lplZGRIkmJjY9WrVy8VFRVp4cKFfi4XAEJbndrS7/tI733MAhoAYDevQdjhcCgpKUmtW7fWjh071Lt3b/Xt27dMnwsuuKDk/qxZs/Tee+8RggHAjb6/lFrEsoAGAAQDr0G4uLhYQ4cO1eLFixUVFaWZM2dq06ZNGjJkiCRp6tSpAS8SAGqKEQPMAhrLV9ldCQAgQpLltVcAOBwOJScn2/HRAGCL7tdLS2ZI/R+XXuOPZgBQbTzlTlaWA4BqMjxN2rWPBTQAIFgQhAGgGlx6kZRyAwtoAEAwIQgDQDUoWUDj33ZXAgBwIggDQIA5F9CYu1A6wAIaABA0CMIAEGAPOhfQmGt3JQCA0gjCABBAdWpLD/+8gMY3+XZXAwAojSAMAAHkXEBjAsspA0DQIQgDQAANT5PW5UjLVtpdCQDgTARhAAiQW66XrmjLcsoAEKwIwgAQICNYQAMAghpBGAACoN2FZgGNyfOkkyygAQBBiSAMAAEwrL90/IT0SobdlQAAPCEIA4CfxTaR+qdKcxawgAYABDOCMAD42UN9WEADAEIBQRgA/Mi5gMb/PmEBDQAIdgRhAPCjPreZBTS4ZBoABD+CMAD40YgBLKABAKGCIAwAfuJcQIPllAEgNBCEAcBPhqdJu/dJb/7P7koAAL4gCAOAH7S7UOp1gzSJBTQAIGQQhAHAD1hAAwBCD0EYAKrIuYDG3IUsoAEAoYQgDABV9GBvFtAAgFBEEAaAKqhTWxraV1q0QsrZZnc1AIDKIAgDQBWwgAYAhC6CMABUwfA0af030kdf2l0JAKCyCMIAcJa6dZKuvJjRYAAIVQRhADhLIwawgAYAhDKCMACchUsuMAtoTH6TBTQAIFQRhAHgLLCABgCEPoIwAFSScwGN1zKl/YfsrgYAcLYIwgBQSQ/2lurVZQENAAh1BGEAqIQ6taWH+5gFNDZvtbsaAEBVEIQBoBLGPCzFNZP+PsPuSgAAVUUQBgAfdbpa+vP90rT/SB+vtrsaAEBVEYQBwAcN6ktzx0nbd0kjxtldDQDAH2rZXQAAhILn/yRdkCjdNED68Zjd1QAA/IERYQDwovv10u/7SBPmSCscdlcDAPAXgjAAVCCmkTTzb9LXedITL9pdDQDAn5gaAQAVeOkJqUVTKfVh6aeTdlcDAPAnRoQBwINf3yrde4c09hVp7Sa7qwEA+BtBGADcaBErvfIXybFBeu5Vu6sBAAQCQRgA3Hj1aalBPan/41JRkd3VAAACgTnCAHCGgb+S7rhZGvaclLPN7moAAIHCiDAAlNKqpfTiSGn5Kulfr9ldDQAgkAjCAPCziAhp9nPm/sBRkmXZWw8AILCYGgEAP/tjP6lrB+m+J6TvdtpdDQAg0BgRBgBJl1wgjRshZS6TZs23uxoAQHUgCAMIe7VqSXPHST8ekwaPsbsaAEB1YWoEgLA3arCUfIX06z9Ke/bbXQ0AoLr4NCLco0cP5eTkKDc3V+np6eVe79u3r9atW6d169bp888/15VXXun3QgEgEK65THryQen1TGn+UrurAQBUN6uiFhkZaeXl5Vlt2rSxoqOjrezsbKtdu3Zl+nTq1MmKiYmxJFk9e/a0Vq5cWeE2JVkOh8NrHxqNRgtkq1tH1tf/lVWwXFZMI/vrodFoNFpgmqfc6XVEuEOHDsrLy1N+fr5OnTqljIwMpaamlunz5Zdf6vDhw5KklStXKiEhwdtmAcB2zzwiXXqRuUrE4e/trgYAUN28BuH4+HgVFBSUPC4sLFR8fLzH/vfff7/ef/99/1QHAAFyQ7I0PE2aPE9a+oXd1QAA7OD1ZLmIiIhyz1kerjLftWtX3X///frFL37h9vVBgwZp8ODBkqTY2NjK1AkAftOwgTT7WWlrgfTnf9hdDQDALl5HhAsLC5WYmFjyOCEhQTt3lr/S/BVXXKHp06crNTVVBw8edLutadOmKTk5WcnJydq/n1OzAdjjhcel88+T0h6Xjh23uxoAgF28BmGHw6GkpCS1bt1a0dHR6t27tzIzM8v0SUxM1Pz589WvXz/l5uYGrFgAqKrbukoP/EZ6fob0Zbbd1QAA7OR1akRxcbGGDh2qxYsXKyoqSjNnztSmTZs0ZMgQSdLUqVP11FNPqWnTpnr55ZclSUVFRUpOTg5s5QBQSU1jpOl/ldblSH+ZZHc1AAC7RchcPqLaORwOwjKAavXvF6Q7u0nX/lbasMXuagAA1cVT7mSJZQBhoXcv6Xcp0lMvEYIBAAZBGECN17K59PJT0hdZ0t9n2l0NACBYEIQB1HgznpFqR0tpI6XTp+2uBgAQLLyeLAcAoWzI3VLPLtLv/yrlfWd3NQCAYMKIMIAa68LzpX/+WVryuTTlTburAQAEG4IwgBopMtKsHneqSLrvCburAQAEI6ZGAKiRHh0o/eIa6Z7HpB177K4GABCMGBEGUONcniSN/aP09mJp3nt2VwMACFYEYQA1SnS09Np46dAR6aGn7a4GABDMmBoBoEYZ83vp6nbS7Q9J+w/ZXQ0AIJgxIgygxuh4lfT4IGnGO9J7H9tdDQAg2BGEAdQI9etJc8dJBbul4c/ZXQ0AIBQwNQJAjTBuhNS2tdS1v/TDUburAQCEAkaEAYS8bp2kP9wrTZgjfeKwuxoAQKggCAMIaY0bSrP+Jm3eKo2aYHc1AIBQwtQIACHtX09I5zWTrustnfjJ7moAAKGEEWEAIeuu7lL/VOmZV6Svvra7GgBAqCEIAwhJzZtKU/8irdko/W2q3dUAAEIRQRhAyLksSVoyXWrYQOr/uFRUZHdFAIBQRBAGEDIiIqRhadKa/0hxsdKdQ81JcgAAnA1OlgMQEuJbSLOfk27pJC38SBr0lLTvoN1VAQBCGUEYQNC7u5c05Skpupb0wGhpxtt2VwQAqAkIwgCCVuOG0uTR0j23S19mS/3Spa3b7a4KAFBTEIQBBKWuHaQ5z0ktm0uj/yU996pUXGx3VQCAmoQgDCCo1I6W/jZMGjFAyv1O6tTHXCINAAB/IwgDCBqXJ0lv/F268mLp5Telx/4uHTtud1UAgJqKIAzAdhER0vA06dnh0qEjUq8h0vsr7K4KAFDTEYQB2CohzswFvvk6acGH5rJo+w/ZXRUAIBwQhAHYps9t0stPSbWipPuflGa+Y3dFAIBwQhAGUO1iGpkA3Oc26Yssc1m0bQV2VwUACDcEYQDV6qaOZipEXKz05ERp3DQuiwYAsAdBGEC1qFPbnAw3YoCUs81cFu2rr+2uCgAQzgjCAALuyoul15+XrmgrTXpD+vM/pOMn7K4KABDuCMIAAiYy0owAP/OIdPCI1HOQtPgzu6sCAMAgCAMIiPNbmrnAXTtI7yyRhoyRDhy2uyoAAFwIwgD87p7bpcmjzYjwgJHSnAV2VwQAQHkEYQB+06SxNGWMdHeK9NlXUv/HpfxCu6sCAMA9gjAAv+jWSZr9rNSiqTRqgjR+unT6tN1VAQDgWaTdBQAIbXXrSBNGSh/OlH44Kl3XW3ruVUIwACD4MSIMoNKioqSOV0opN5hpEEmtpJdel9L/yWXRAAChgyAMwCfNm0o9f2HC763XS+fGSEVF0hfZ0tCx0pLP7a4QAIDKIQgDcCsyUupwpdTrBimli3Tt5eb5XfukBR9J738qLf1COvKDvXUCAHC2CMIASjQ7V+rxCxN+b+0sNY2RioulL7OlJ16UFq2Q1uVIlmV3pQAAVB1BGAhjkZFmpLf0qG9kpLRnv/Tf5a5R30NH7K4UAAD/IwgDYaZpjBn1Teki9ewixTYxo76r1ktjJkmLPpGyNjPqCwCo+QjCQA0XEWFGelO6mBPdOlxhRn33HjBTHd5fIS35QjrI8scAgDBDEAZqoHNjzJUdet0o9ehsrvhw+rS0eoP09GQTgL/6mlFfAEB4IwgDNUDtaOmKtmbEN6WLucZvVJS0/5D0wadmru/iz6QDjPoCAFCCIAwEubp1pPgWUkILKSFOSowzt87HCS2kFrGm7+nT0pqN0jOvmFHfNRtZ4Q0AAE8IwoCN6tcrG2gTSoVcZ+CNbVL+fQcPS4V7pILdJuwW7pG2bpc+/FLad7D6vw4AAEIRQRgIkHPqnxFszzsj9LYwc3nPtO+gVLjbhNwv17nuF+42gXfHHunY8er/egAAqGl8CsI9evTQxIkTFRUVpenTp2v8+PHl+kycOFG9evXSsWPHNGDAAGVlZfm9WCCQ6tWVGtQzo7QN6kn160oN6v986+vz9cw0hcQ4qXHD8p+xZ78Js9sKpBVrXOG2dMg98VP1f+0AAIQjr0E4MjJSkydPVvfu3VVYWCiHw6HMzExt3ry5pE9KSoqSkpKUlJSkjh07asqUKbruuusCWjiCT2SkuVRXRIQU+fNtVJRUK0qqVUuKinTdL3Pr4X7Jez31LfXcmX1rR5tg6i28ln6tsk78ZEZmjx6Xjp34+fa4tOVbadnKn6cu7HIF3Z17pZOn/L7bAQDAWfIahDt06KC8vDzl5+dLkjIyMpSamlomCKempmru3LmSpFWrVikmJkZxcXHavXt3gMo+O82bSgsmlX8+IsLNc27e77afj885n3e+FnFG39K37p5zvsfja6Xf72bbERE/B1WVul8qsLoLsSXPqVR/d9uIdP/12u3osbIB1RlY9x2Sju5w/5qn9xw9Vvb+8Z/MIhQAACB0eQ3C8fHxKigoKHlcWFiojh07eu0THx9fLggPGjRIgwcPliTFxsZWqfCzYVnS9z96fs2n56rwXufzztfK3frSxyrbz2OfM7Z5+rTr+dPO2wqecz6u8LlS23U+PnO7RcUmMBY5W1HF99329fCeYi/bAQAAqIjXIBzhZnjTOiPl+dJHkqZNm6Zp06ZJkhwOh89F+su+g1LPQdX+sQAAAAhCXv+oXVhYqMTExJLHCQkJ2rlzZ6X7AAAAAMHEaxB2OBxKSkpS69atFR0drd69eyszM7NMn8zMTPXv31+S1LFjRx05ciTo5gcDAAAApXmdGlFcXKyhQ4dq8eLFioqK0syZM7Vp0yYNGTJEkjR16lQtWrRIvXr1Ul5eno4dO6aBAwcGvHAAAACgKiLk/vyvgHM4HEpOTrbjowEAABBGPOXOIL3wFQAAABBYBGEAAACEJYIwAAAAwhJBGAAAAGGJIAwAAICwRBAGAABAWCIIAwAAICwRhAEAABCWCMIAAAAIS7atLLd371599913dny0YmNjtX//fls+OxSxvyqH/VU57K/KYX9VDvurcthflcP+qhw791erVq3UvHlzt69Z4dYcDoftNYRSY3+xv9hfwdPYX+wv9lfwNPZX6O8vpkYAAAAgLBGEAQAAEJaiJP3F7iLssHbtWrtLCCnsr8phf1UO+6ty2F+Vw/6qHPZX5bC/KifY9pdtJ8sBAAAAdmJqBAAAAMJSjQzCv/nNb7Rx40YVFxfrmmuuKfPa448/rtzcXOXk5OjWW291+/4mTZpoyZIl2rJli5YsWaKYmJjqKDtoZGRkKCsrS1lZWcrPz1dWVpbbfvn5+Vq/fr2ysrLkcDiqucrgMWbMGBUWFpbss5SUFLf9evTooZycHOXm5io9Pb2aqwwezz//vDZv3qx169Zp/vz5aty4sdt+4X58+XK8TJw4Ubm5uVq3bp3at29fzRUGj4SEBC1btkybNm3Sxo0b9cc//rFcnxtvvFGHDx8u+TkdPXq0DZUGD19+vji+jLZt25YcN1lZWTpy5IgeeeSRMn3C/fiaMWOG9uzZow0bNpQ852uWCob/G22/dIW/2yWXXGK1bdvWWr58uXXNNdeUPN+uXTsrOzvbql27ttW6dWsrLy/PioyMLPf+8ePHW+np6ZYkKz093Ro3bpztX5Nd7R//+Ic1evRot6/l5+dbTZs2tb1Gu9uYMWOsRx99tMI+kZGRVl5entWmTRsrOjrays7Ottq1a2d77Xa07t27W1FRUZYka9y4cR5/vsL5+PLleElJSbEWLVpkSbI6duxorVy50va67WpxcXFW+/btLUnWOeecY33zzTfl9teNN95o/fe//7W91mBp3n6+OL7ct8jISGvXrl3W+eefX+b5cD++unTpYrVv397asGFDyXO+ZKlg+L+xRo4I5+TkaMuWLeWeT01NVUZGhk6ePKlvv/1WeXl56tChg9t+c+bMkSTNmTNHd955Z8BrDla/+93v9Oabb9pdRsjr0KGD8vLylJ+fr1OnTikjI0Opqal2l2WLpUuXqri4WJK0cuVKJSQk2FxR8PHleElNTdXcuXMlSatWrVJMTIzi4uLsKNd2u3fvLvnL1Y8//qjNmzcrPj7e5qpCG8eXe926ddPWrVu1fft2u0sJKp9++qkOHjxY5jlfslQw/N9YI4OwJ/Hx8SooKCh5XFhY6PYfyxYtWmj37t2SzD+wnlYiqem6dOmiPXv2KC8vz+3rlmVpyZIlWrNmjQYNGlTN1QWXoUOHat26dZoxY4bbP//4euyFm/vuu0/vv/++29fC+fjy5XjhmHKvVatWat++vVatWlXutU6dOik7O1uLFi3SpZdeakN1wcPbzxfHl3u9e/f2ODjE8VWWL1kqGI6zWtX6aX60dOlSt7+dPvHEE8rMzHT7noiIiHLPWZbl99pCgS/7r0+fPhWOBnfu3Fm7du1Ss2bNtHTpUuXk5OjTTz8NWM12qmh/TZkyRWPHjpVlWRo7dqz++c9/6v777y/TL9yOPV+Or1GjRqmoqEhvvPGG222E0/F1Jl+Ol3A7pnzRoEEDvfPOOxo2bJh++OGHMq+tXbtWrVq10tGjR5WSkqIFCxaobdu2NlVqP28/Xxxf5UVHR+uOO+7QyJEjy73G8XV2guE4C9kg3L1790q/p7CwUImJiSWPExIStHPnznL99uzZo7i4OO3evVtxcXHau3dvlWoNRt72X1RUlH71q1+VO9mwtF27dkmS9u3bp3fffVcdOnSosUHF1+Nt2rRpeu+998o97+uxV1N421/9+/fXL3/5S3Xr1s1jn3A6vs7ky/ESbseUN7Vq1dI777yjN954Q++++26510sH4/fff18vv/yymjZtqgMHDlRnmUHD288Xx1d5KSkpWrt2rdtMwPFVni9ZKhiOs7CaGpGZmanevXurdu3aat26tZKSkrR69Wq3/dLS0iRJaWlpWrhwYXWXartbbrlFOTk52rFjh9vX69evr3POOafk/q233qqNGzdWZ4lBo/TI51133eV2PzgcDiUlJal169aKjo5W7969Pf7loqbr0aOH0tPTdccdd+j48eNu+4T78eXL8ZKZman+/ftLkjp27KgjR46U/BkyHM2YMUObN2/WhAkT3L7eokWLkvvJycmKjIwM25Diy88Xx1d5Ff2VlOOrPF+yVLD832j72Yb+bnfeeadVUFBgnThxwtq9e7f1wQcflLw2atQoKy8vz8rJybF69uxZ8vy0adNKrjBx7rnnWh9++KG1ZcsW68MPP7SaNGli+9dU3W3WrFnWkCFDyjx33nnnWf/73/8sSVabNm2s7OxsKzs729q4caM1atQo22u2q82dO9dav369tW7dOmvhwoVWXFxcuf0lmbOwv/nmGysvLy+s91dubq61fft2Kysry8rKyrKmTJlSbn9xfLk/XoYMGVLm53LSpElWXl6etX79+jJXyAm31rlzZ8uyLGvdunUlx1VKSkqZ/fXwww9bGzdutLKzs60vv/zS6tSpk+1129U8/XxxfHlu9erVs/bv3281atSo5DmOL1ebN2+etXPnTuvkyZNWQUGBdd9993nMUsH2fyMrywEAACAshdXUCAAAAMCJIAwAAICwRBAGAABAWCIIAwAAICwRhAEAABCWCMIAAAAISwRhAAhW+fmSZfnWZs2yu1oACDkhu8QyANR4L74oxcR4fr1+fWnECKlWLSmMVt4DAH9hQQ0ACFVvvSX99rfS22+bWwBApTA1AgBC0V//asLv2rVS//52VwMAIYkRYQAINXffLWVkSLt2ScnJ0o4ddlcEACGJIAwAoSQ5WfrkE3O/a1dp9WpbywGAUMbJcgAQKlq2lBYskOrVk+65hxAMAFXEHGEACAX16kmZmSYMP/usNG+e3RUBQMhjagQAhALnFSIWLJDuusvuagCgRmBEGACC3dNPmxC8bp107712VwMANQYjwgAQzJxXiNizR+rQQdq+3e6KAKDGIAgDQLC69lppxQopMlK6+Wbpiy/srggAahSCMAAEo3POkb75xpwct3q1tGhRxf2//VaaM6daSgOAmoIgDADBqFUrE2599fHH0k03BaoaAKiRCMIAAAAIS1w1AgAAAGGJIAwAAICwRBAGAABAWCIIAwAAICwRhAEAABCWCMIAAAAISwRhAAAAhCWCMAAAAMISQRgAAABhiSAMAACAsPT/CDtMRCKJL3gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nums = np.arange(-10, 11, step=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.plot(nums, sigmoid(nums), '-', color = 'gold' )\n",
    "plt.xlabel('Z', color='aqua',fontsize=20)\n",
    "\n",
    "plt.title('Zigmoid Function', color='aqua',fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "#### 1.2.2 Cost function and gradient (10pt)\n",
    "\n",
    "Now you will implement the cost function and gradient for logistic regression. Before proceeding we add the intercept term to X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete the code for the function `costFunction` to return the cost and gradient. Recall that the cost function in logistic regression is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\theta\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\theta\\left( x^{(i)} \\right) \\right) \\right]$$\n",
    "\n",
    "and the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$\n",
    "element (for $j = 0, 1, \\cdots , n$) is defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} $$\n",
    "\n",
    "Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$.\n",
    "<a id=\"costFunction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `y(m,) `  is a 1-D aray `array([0, 1, 2, 3])` (like a matrix of `(1,m)`)\n",
    "\n",
    "### The operator between `h(m,) `and `X(m,n)` must be `@` and not `*` and the result is `(m,)` and has sum of all multplied values insdie it (like matrix multiplication)\n",
    "\n",
    "### `y(m,) @ h(m,)`  the result is a number (sum of all multiplied values) also it can be done by `np.sum(y * h)`\n",
    "\n",
    "### `X(m,n) @ y(n,) ` is correct and not `X.T(n,m) @ y(n,) `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to numpy a 1D array has only 1 dimension and all checks are done against that dimension. Because of this we find that np.dot(A,B) checks second dimension of A against the one dimension of B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in A, which has shape (3,2), we have consumed the last part (2,) to perform sum-product. The un-consumed part of A's shape is (3,), and hence the shape of the result of np.dot(A,B), would be (3,). To understand this further, if we take a different example in which A has a shape of (3,4,2), instead of (3,2), the un-consumed part of A's shape would be (3,4,), and the result of np.dot(A,B) would be (3,4,) instead of (3,) which your example produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (4, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "y = np.arange(4,12).reshape(4,2)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9],\n",
       "       [10, 11]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52, 58])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        The parameters for logistic regression. This a vector\n",
    "        of shape (n+1, ).\n",
    "    \n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n+1) where m is the total number\n",
    "        of data points and n is the number of features. We assume the \n",
    "        intercept has already been added to the input.\n",
    "    \n",
    "    y : arra_like\n",
    "        Labels for the input. This is a vector of shape (m, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (n+1, ) which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of theta. You should set J to \n",
    "    the cost. Compute the partial derivatives and set grad to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in theta.\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    cost = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    m = X.shape[0]\n",
    "    h = sigmoid(X @ (theta))\n",
    "    \n",
    "    cost = (-1 / m) * (y @ (np.log(h)) + (1 - y) @ (np.log(1 - h)))\n",
    "    #y and h are treated like matrix despite the fact that theye both are (100,) arrays. like (1,100)\n",
    "    # so we dont need the sum() operator\n",
    "    #or:\n",
    "    # np.sum( y*(np.log(h))+(1-y)*(np.log(1-h)) ) \n",
    "    #y and h are arrays and their multiply is an array so we should sum() these elements\n",
    "\n",
    "    grad = (1 / m) * (h - y) @ X\n",
    "    # h is (m,) and X(m,n) , the operator between them must be @ and not *\n",
    "    # =============================================================\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "      <th>Admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>83.489163</td>\n",
       "      <td>48.380286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>42.261701</td>\n",
       "      <td>87.103851</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>99.315009</td>\n",
       "      <td>68.775409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>55.340018</td>\n",
       "      <td>64.931938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>74.775893</td>\n",
       "      <td>89.529813</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Exam 1     Exam 2  Admitted\n",
       "0   34.623660  78.024693         0\n",
       "1   30.286711  43.894998         0\n",
       "2   35.847409  72.902198         0\n",
       "3   60.182599  86.308552         1\n",
       "4   79.032736  75.344376         1\n",
       "..        ...        ...       ...\n",
       "95  83.489163  48.380286         1\n",
       "96  42.261701  87.103851         1\n",
       "97  99.315009  68.775409         1\n",
       "98  55.340018  64.931938         1\n",
       "99  74.775893  89.529813         1\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "      <th>Admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>83.489163</td>\n",
       "      <td>48.380286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>42.261701</td>\n",
       "      <td>87.103851</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>99.315009</td>\n",
       "      <td>68.775409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>55.340018</td>\n",
       "      <td>64.931938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>74.775893</td>\n",
       "      <td>89.529813</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ones     Exam 1     Exam 2  Admitted\n",
       "0      1  34.623660  78.024693         0\n",
       "1      1  30.286711  43.894998         0\n",
       "2      1  35.847409  72.902198         0\n",
       "3      1  60.182599  86.308552         1\n",
       "4      1  79.032736  75.344376         1\n",
       "..   ...        ...        ...       ...\n",
       "95     1  83.489163  48.380286         1\n",
       "96     1  42.261701  87.103851         1\n",
       "97     1  99.315009  68.775409         1\n",
       "98     1  55.340018  64.931938         1\n",
       "99     1  74.775893  89.529813         1\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a ones column - this makes the matrix multiplication work out easier( X0 = 1)\n",
    "data.insert(0, 'Ones', 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 3), (100, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones     Exam 1     Exam 2\n",
       "0     1  34.623660  78.024693\n",
       "1     1  30.286711  43.894998\n",
       "2     1  35.847409  72.902198\n",
       "3     1  60.182599  86.308552\n",
       "4     1  79.032736  75.344376"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays and initalize the parameter array theta\n",
    "X = np.array(X.values)\n",
    "y = np.array(y.values)\n",
    "initial_theta = np.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , 34.62365962, 78.02469282],\n",
       "       [ 1.        , 30.28671077, 43.89499752],\n",
       "       [ 1.        , 35.84740877, 72.90219803],\n",
       "       [ 1.        , 60.18259939, 86.3085521 ],\n",
       "       [ 1.        , 79.03273605, 75.34437644],\n",
       "       [ 1.        , 45.08327748, 56.31637178],\n",
       "       [ 1.        , 61.10666454, 96.51142588],\n",
       "       [ 1.        , 75.02474557, 46.55401354],\n",
       "       [ 1.        , 76.0987867 , 87.42056972],\n",
       "       [ 1.        , 84.43281996, 43.53339331]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 3), (100, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , y.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's mentioned in the defenition of the costFunction() that y should be an (m, ) array\n",
    "\n",
    "- The next cell does this conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.squeeze(y) #This line converts y from (100, 1) to (100, )\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)#now X is a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , 34.62365962, 78.02469282],\n",
       "       [ 1.        , 30.28671077, 43.89499752],\n",
       "       [ 1.        , 35.84740877, 72.90219803],\n",
       "       [ 1.        , 60.18259939, 86.3085521 ],\n",
       "       [ 1.        , 79.03273605, 75.34437644]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done call your `costFunction` using two test cases for  $\\theta$ by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial theta (zeros): 0.693\n",
      "Gradient at initial theta (zeros):\n",
      "\t[-0.1000, -12.0092, -11.2628]\n",
      "Cost at test theta: 0.218\n",
      "Gradient at test theta:\n",
      "\t[0.043, 2.566, 2.647]\n"
     ]
    }
   ],
   "source": [
    "cost, grad = costFunction(initial_theta, X, y)\n",
    "\n",
    "print('Cost at initial theta (zeros): {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "print('Gradient at initial theta (zeros):')\n",
    "print('\\t[{:.4f}, {:.4f}, {:.4f}]'.format(*grad))\n",
    "\n",
    "\n",
    "# Compute and display cost and gradient with non-zero theta\n",
    "test_theta = np.array([-24, 0.2, 0.2])\n",
    "cost, grad = costFunction(test_theta, X, y)\n",
    "\n",
    "print('Cost at test theta: {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "print('Gradient at test theta:')\n",
    "print('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((), (3,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost.shape, grad.shape\n",
    "\n",
    "#cost is a number, shape:()\n",
    "#grad is a 1-D array, shape:(3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Learning parameters using `scipy.optimize` (5 pt)\n",
    "\n",
    "In the previous assignment, you found the optimal parameters of a linear regression model by implementing gradient descent. You wrote a cost function and calculated its gradient, then took a gradient descent step accordingly. This time, instead of taking gradient descent steps, you will use the [`scipy.optimize` module](https://docs.scipy.org/doc/scipy/reference/optimize.html). SciPy is a numerical computing library for `python`. It provides an optimization module for root finding and minimization. As of `scipy 1.0`, the function `scipy.optimize.minimize` is the method to use for optimization problems(both constrained and unconstrained).\n",
    "\n",
    "For logistic regression, you want to optimize the cost function $J(\\theta)$ with parameters $\\theta$.\n",
    "Concretely, you are going to use `optimize.minimize` to find the best parameters $\\theta$ for the logistic regression cost function, given a fixed dataset (of X and y values). You will pass to `optimize.minimize` the following inputs:\n",
    "- `costFunction`: A cost function that, when given the training set and a particular $\\theta$, computes the logistic regression cost and gradient with respect to $\\theta$ for the dataset (X, y). It is important to note that we only pass the name of the function without the parenthesis. This indicates that we are only providing a reference to this function, and not evaluating the result from this function.\n",
    "- `initial_theta`: The initial values of the parameters we are trying to optimize.\n",
    "- `(X, y)`: These are additional arguments to the cost function.\n",
    "- `jac`: Indication if the cost function returns the Jacobian (gradient) along with cost value. (True)\n",
    "- `method`: Optimization method/algorithm to use\n",
    "- `options`: Additional options which might be specific to the specific optimization method. In the following, we only tell the algorithm the maximum number of iterations before it terminates.\n",
    "\n",
    "If you have completed the `costFunction` correctly, `optimize.minimize` will converge on the right optimization parameters and return the final values of the cost and $\\theta$ in a class object. Notice that by using `optimize.minimize`, you did not have to write any loops yourself, or set a learning rate like you did for gradient descent. This is all done by `optimize.minimize`: you only needed to provide a function calculating the cost and the gradient.\n",
    "\n",
    "In the following, we already have code written to call `optimize.minimize` with the correct arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: TNC\n",
      "Cost at theta found by opt.minimize: 0.203\n",
      "theta:\n",
      "\t[-25.161, 0.206, 0.201]\n",
      "method: BFGS\n",
      "Cost at theta found by opt.minimize: 0.203\n",
      "theta:\n",
      "\t[-25.161, 0.206, 0.201]\n",
      "method: CG\n",
      "Cost at theta found by opt.minimize: 0.203\n",
      "theta:\n",
      "\t[-25.164, 0.206, 0.201]\n",
      "method: Newton-CG\n",
      "Cost at theta found by opt.minimize: 0.203\n",
      "theta:\n",
      "\t[-25.136, 0.206, 0.201]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Behnam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Behnam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in matmul\n"
     ]
    }
   ],
   "source": [
    "import scipy \n",
    "options= {'maxiter': 400}\n",
    "estimators = ['TNC','BFGS', 'CG', 'Newton-CG' ]\n",
    "\n",
    "\n",
    "for estimator in estimators:\n",
    "    res = scipy.optimize.minimize(fun = costFunction,\n",
    "                            x0 = initial_theta,\n",
    "                            args=(X, y),\n",
    "                            jac = True,\n",
    "                            options = options,\n",
    "                            method = estimator \n",
    "                               )  \n",
    "\n",
    "# the fun property of `OptimizeResult` object returns\n",
    "# the value of costFunction at optimized theta\n",
    "    cost = res.fun\n",
    "\n",
    "# the optimized theta is in the x property\n",
    "    theta = res.x\n",
    "\n",
    "# Print theta to screen\n",
    "    print(f\"method: {estimator}\")\n",
    "    print('Cost at theta found by opt.minimize: {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "    print('theta:')\n",
    "    print('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*theta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "#### 1.2.4 Evaluating logistic regression (10 pt)\n",
    "\n",
    "After learning the parameters, you can use the model to predict whether a particular student will be admitted. Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. In this part, your task is to complete the code in function `predict`. The predict function will produce “1” or “0” predictions given a dataset and a learned parameter vector $\\theta$. \n",
    "<a id=\"predict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-25.1358504 ,   0.20602723,   0.20126608])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach that you can set the threshold with desired values other than 0.5\n",
    "\n",
    "\n",
    "    sklearn.preprocessing.binarize(list(zip(hypo, 1-hypo)), threshold=0.5)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression.\n",
    "    Computes the predictions for X using a threshold at 0.5 \n",
    "    (i.e., if sigmoid(theta.T*x) >= 0.5, predict 1)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Parameters for logistic regression. A vecotor of shape (n+1, ).\n",
    "    \n",
    "    X : array_like\n",
    "        The data to use for computing predictions. The rows is the number \n",
    "        of points to compute predictions, and columns is the number of\n",
    "        features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : array_like\n",
    "        Predictions and 0 or 1 for each row in X. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the following code to make predictions using your learned \n",
    "    logistic regression parameters.You should set p to a vector of 0's and 1's    \n",
    "    \"\"\"\n",
    "    m = X.shape[0] # Number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    p = np.zeros(m)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    p = np.round(sigmoid(X@(theta.T)))\n",
    "    #OR:\n",
    "    #import sklearn\n",
    "    #sklearn.preprocessing.binarize(list(zip(hypo, 1-hypo)), threshold=0.5)[:,0]\n",
    "    # ============================================================\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the code in `predict`, we proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 3), (3,), (100,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a student with scores 45 and 85,we predict an admission probability of 0.776\n",
      "Train Accuracy: 89.00 %\n"
     ]
    }
   ],
   "source": [
    "#  Predict probability for a student with score 45 on exam 1 \n",
    "#  and score 85 on exam 2 \n",
    "prob = sigmoid(np.dot([1, 45, 85], theta))\n",
    "print('For a student with scores 45 and 85,'\n",
    "      'we predict an admission probability of {:.3f}'.format(prob))\n",
    "\n",
    "\n",
    "# Compute accuracy on our training set\n",
    "\n",
    "p = np.matrix(predict(theta, X))\n",
    "print('Train Accuracy: {:.2f} %'.format(np.mean(p == y) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n",
    "#It can be treated like (1,100) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Regularized logistic regression (40 pt)\n",
    "\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n",
    "\n",
    "First, we load the data from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test 1</th>\n",
       "      <th>Test 2</th>\n",
       "      <th>Accepted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.68494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.69225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.50219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.46564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Test 1   Test 2  Accepted\n",
       "0  0.051267  0.69956         1\n",
       "1 -0.092742  0.68494         1\n",
       "2 -0.213710  0.69225         1\n",
       "3 -0.375000  0.50219         1\n",
       "4 -0.513250  0.46564         1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "path =  r'C:\\Users\\Behnam\\Downloads\\Python\\Datasets\\ML_HW2\\ex2data2.txt'\n",
    "data2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Visualizing the data (5 pt)\n",
    "\n",
    "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. Write codes below to display a figure where the axes are the two exam scores, and the positive and negative examples are shown with different markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Accepted = data2.loc[data2.Accepted == 1]\n",
    "# filter out the Admitted Stuents \n",
    "\n",
    "not_Accepted = data2.loc[data2.Accepted == 0]\n",
    "# filter out the not Admitted Students\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(num=None, figsize=(10, 6), dpi=60,)\n",
    "plt.scatter(Accepted['Test 1'], Accepted['Test 2'], s=10, label='Accepted', linewidth=5, color = 'magenta')\n",
    "plt.scatter(not_Accepted['Test 1'], not_Accepted['Test 2'], s=10, label='Not Accepted', linewidth=5, color ='lawngreen') \n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Test 1', color='aqua',fontsize=20)\n",
    "plt.ylabel('Test 2', color='aqua',fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight-forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.\n",
    "\n",
    "### 2.2 Feature mapping (10 pt)\n",
    "\n",
    "One way to fit the data better is to create more features from each data point. Next, we will map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power:\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & x_1 & x_2 & x_1^2 & x_1 x_2 & x_2^2 & x_1^3 & \\dots & x_1 x_2^5 & x_2^6 \\end{bmatrix} $$\n",
    "\n",
    "As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\n",
    "While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n",
    "\n",
    "Write codes below to create a new dataframe with all 28 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test 1</th>\n",
       "      <th>Test 2</th>\n",
       "      <th>Accepted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.699560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.684940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.692250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.502190</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.465640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>-0.720620</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>-0.593890</td>\n",
       "      <td>0.494880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>-0.484450</td>\n",
       "      <td>0.999270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>-0.006336</td>\n",
       "      <td>0.999270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.632650</td>\n",
       "      <td>-0.030612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test 1    Test 2  Accepted\n",
       "0    0.051267  0.699560         1\n",
       "1   -0.092742  0.684940         1\n",
       "2   -0.213710  0.692250         1\n",
       "3   -0.375000  0.502190         1\n",
       "4   -0.513250  0.465640         1\n",
       "..        ...       ...       ...\n",
       "113 -0.720620  0.538740         0\n",
       "114 -0.593890  0.494880         0\n",
       "115 -0.484450  0.999270         0\n",
       "116 -0.006336  0.999270         0\n",
       "117  0.632650 -0.030612         0\n",
       "\n",
       "[118 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X2 and y2 out the data dataframe\n",
    "X2 = data2[['Test 1', 'Test 2']]\n",
    "y2 = data2['Accepted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X2 and y2 into np.arrays\n",
    "X2 = np.array(X2.values)\n",
    "y2 = np.array(y2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118, 2), (118,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.092742,  0.68494 ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function adds features of differents degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`degree = 7\n",
    "x1 = data2['Test 1']\n",
    "x2 = data2['Test 2']\n",
    "for i in range(0, degree):\n",
    "    for j in range(0, i+1):\n",
    "        data2['F' + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)\n",
    "data2.drop('Test 1', axis=1, inplace=True)\n",
    "data2.drop('Test 2', axis=1, inplace=True)\n",
    "data2.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        , -0.092742  ,  0.68494   ,  0.00860108, -0.06352271,\n",
       "        0.4691428 , -0.00079768,  0.00589122, -0.04350924,  0.32133467,\n",
       "        0.00007398, -0.00054636,  0.00403513, -0.02980122,  0.22009497,\n",
       "       -0.00000686,  0.00005067, -0.00037423,  0.00276382, -0.02041205,\n",
       "        0.15075185,  0.00000064, -0.0000047 ,  0.00003471, -0.00025632,\n",
       "        0.00189305, -0.01398103,  0.10325597])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(6) #Creates a object for new features upto degree of 6 out of initial features(for X)\n",
    "X2 = poly.fit_transform(X2) # Apply this new object to X\n",
    "# Note that this function inserts a column with 'ones' in the X array for the intercept.\n",
    "\n",
    "X2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 28)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have 28 features including a column of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "### 2.3 Cost function and gradient (15 pt)\n",
    "\n",
    "Now you will implement code to compute the cost function and gradient for regularized logistic regression. Complete the code for the function `costFunctionReg` below to return the cost and gradient.\n",
    "\n",
    "The regularized cost function in logistic regression is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)}\\log \\left( h_\\theta \\left(x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
    "\n",
    "Note that you should not regularize the parameters $\\theta_0$. The gradient of the cost function is a vector where the $j^{th}$ element is defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\qquad \\text{for } j =0 $$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\qquad \\text{for } j \\ge 1 $$\n",
    "<a id=\"costFunctionReg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Logistic regression parameters. A vector with shape (n, ). n is \n",
    "        the number of features including any intercept. If we have mapped\n",
    "        our initial features into polynomial features, then n is the total \n",
    "        number of polynomial features. \n",
    "    \n",
    "    X : array_like\n",
    "        The data set with shape (m x n). m is the number of examples, and\n",
    "        n is the number of features (after feature mapping).\n",
    "    \n",
    "    y : array_like\n",
    "        The data labels. A vector with shape (m, ).\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the regularized cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (n, ) which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost `J` of a particular choice of theta.\n",
    "    Compute the partial derivatives and set `grad` to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in theta.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    cost = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "    m = X.shape[0]\n",
    "    h = sigmoid(X @ (theta)) #first we calculate h with give theta\n",
    "    theta[0] = 0 #then we set theta[0]=0 Because we want to make regularization term for theta[0] = 0\n",
    "  \n",
    "    cost = (-1 / m) * (y@(np.log(h)) + (1 - y)@(np.log(1 - h))) + lambda_/(2*m)*np.sum(np.square(theta))\n",
    "\n",
    "    grad = (1 / m) * (h - y) @ X + (lambda_/m)*theta\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check all the dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118,), (118,), (118, 28), (28,), (28,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_theta = np.zeros(X2.shape[1])\n",
    "h = sigmoid(X2 @ (initial_theta))\n",
    "m = y2.size\n",
    "\n",
    "grad = (1 / m) * (h - y2) @ X2 + (1/m)*initial_theta\n",
    "h.shape, y2.shape, X2.shape, grad.shape, initial_theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done with the `costFunctionReg`, we call it below using the initial value of $\\theta$ (initialized to all zeros), and also another test case where $\\theta$ is all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial theta (zeros): 0.693\n",
      "Gradient at initial theta (zeros) - first five values only:\n",
      "\t[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cost at test theta (Ones)   : 3.16\n",
      "Gradient at test theta (Ones) - first five values only:\n",
      "\t[0.3460, 0.1614, 0.1948, 0.2269, 0.0922]\n"
     ]
    }
   ],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(X2.shape[1])\n",
    "\n",
    "# Set regularization parameter lambda to 1\n",
    "# DO NOT use `lambda` as a variable name in python\n",
    "# because it is a python keyword\n",
    "lambda_ = 1\n",
    "\n",
    "# Compute and display initial cost and gradient for regularized logistic\n",
    "# regression\n",
    "cost, grad = costFunctionReg(initial_theta, X2, y2, lambda_)\n",
    "\n",
    "print('Cost at initial theta (zeros): {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "print('Gradient at initial theta (zeros) - first five values only:')\n",
    "print('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\n",
    "\n",
    "\n",
    "\n",
    "# Compute and display cost and gradient\n",
    "# with all-ones theta and lambda = 10\n",
    "test_theta = np.ones(X2.shape[1])\n",
    "cost, grad = costFunctionReg(test_theta, X2, y2, 10)\n",
    "\n",
    "print('------------------------------------------------------------\\n')\n",
    "print('Cost at test theta (Ones)   : {:.2f}'.format(cost))\n",
    "\n",
    "\n",
    "print('Gradient at test theta (Ones) - first five values only:')\n",
    "print('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Learning parameters using `scipy.optimize.minimize` (5 pt)\n",
    "\n",
    "Similar to the previous parts, you will use `opt.minimize` to learn the optimal parameters $\\theta$. If you have completed the cost and gradient for regularized logistic regression (`costFunctionReg`) correctly, you should be able to step through the next part of to learn the parameters $\\theta$ using `opt.minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Accuracy for TNC\n",
      "lambda = 0.1: 66.1 %\n",
      "lambda = 100: 53.4 %\n",
      "lambda =   1: 66.1 %\n",
      "------------------------------------------------\n",
      "Accuracy for CG\n",
      "lambda = 0.1: 83.9 %\n",
      "lambda = 100: 61.0 %\n",
      "lambda =   1: 83.1 %\n",
      "------------------------------------------------\n",
      "Accuracy for Newton-CG\n",
      "lambda = 0.1: 56.8 %\n",
      "lambda = 100: 53.4 %\n",
      "lambda =   1: 55.1 %\n",
      "------------------------------------------------\n",
      "Accuracy for L-BFGS-B\n",
      "lambda = 0.1: 41.5 %\n",
      "lambda = 100: 53.4 %\n",
      "lambda =   1: 55.1 %\n",
      "------------------------------------------------\n",
      "Accuracy for BFGS\n",
      "lambda = 0.1: 83.9 %\n",
      "lambda = 100: 61.0 %\n",
      "lambda =   1: 83.1 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Behnam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Behnam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in matmul\n",
      "C:\\Users\\Behnam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "# set options for optimize.minimize\n",
    "options= {'maxiter': 400}\n",
    "\n",
    "# see documention for scipy's optimize.minimize  for description about\n",
    "# the different parameters\n",
    "# The function returns an object `OptimizeResult`\n",
    "# We use truncated Newton algorithm for optimization\n",
    "\n",
    "#lambda_ = 1\n",
    "# ===================== YOUR CODE HERE ======================\n",
    "import scipy\n",
    "options= {'maxiter': 400}\n",
    "methods = ['TNC', 'CG', 'Newton-CG', 'L-BFGS-B' ,'BFGS']\n",
    "lambda_s = [0.1,  100, 1]\n",
    "\n",
    "for method in methods:\n",
    "    print('------------------------------------------------')\n",
    "    print(f'Accuracy for {method}')\n",
    "    for lambda_ in lambda_s:\n",
    "        res = scipy.optimize.minimize(fun = costFunctionReg,\n",
    "                                    x0 = initial_theta,\n",
    "                                    args=(X2, y2, lambda_),\n",
    "                                    jac = True,\n",
    "                                    options = options,\n",
    "                                    method = method\n",
    "                                       )\n",
    "        cost = res.fun\n",
    "        theta = res.x\n",
    "        p2 = np.matrix(predict(theta, X2))\n",
    "        \n",
    "        print(f'lambda = {lambda_:>3}: %.1f %%' % (np.mean(p2 == y2) * 100))\n",
    "     \n",
    "    # =============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, increasing $\\lambda$ will decrease the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.27268739,  0.62557016,  1.1809665 , -2.01919822, -0.91761468,\n",
       "       -1.43194199,  0.12375921, -0.36513086, -0.35703388, -0.17485805,\n",
       "       -1.45843772, -0.05129676, -0.61603963, -0.2746414 , -1.19282569,\n",
       "       -0.24270336, -0.20570022, -0.04499768, -0.27782709, -0.29525851,\n",
       "       -0.45613294, -1.04377851,  0.02762813, -0.29265642,  0.01543393,\n",
       "       -0.32759318, -0.14389199, -0.92460119])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### 2.5 Evaluating logistic regression (5 pt)\n",
    "\n",
    "In this part, your task is to complete the code in function `predict`. The predict function will produce “1” or “0” predictions given a dataset and a learned parameter vector $\\theta$. \n",
    "<a id=\"predict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression.\n",
    "    Computes the predictions for X using a threshold at 0.5 \n",
    "    (i.e., if sigmoid(theta.T*x) >= 0.5, predict 1)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Parameters for logistic regression. A vecotor of shape (n+1, ).\n",
    "    \n",
    "    X : array_like\n",
    "        The data to use for computing predictions. The rows is the number \n",
    "        of points to compute predictions, and columns is the number of\n",
    "        features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : array_like\n",
    "        Predictions and 0 or 1 for each row in X. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the following code to make predictions using your learned \n",
    "    logistic regression parameters.You should set p to a vector of 0's and 1's    \n",
    "    \"\"\"\n",
    "    m = X.shape[0] # Number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    p = np.zeros(m)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    p = np.round(sigmoid(X@(theta.T)))\n",
    "    \n",
    "    # ============================================================\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the code in `predict`, we proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118, 28), (28,), (118,))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape, theta.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 83.1 %\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy on our training set\n",
    "p2 = np.matrix(predict(theta, X2))\n",
    "print('Train Accuracy: %.1f %%' % (np.mean(p2 == y2) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Optional (ungraded) exercises\n",
    "\n",
    "In this part of the exercise, you can try out different regularization parameters for the dataset to understand how regularization prevents overfitting.\n",
    "\n",
    "Notice the changes in the decision boundary as you vary $\\lambda$. With a small\n",
    "$\\lambda$, you should find that the classifier gets almost every training example correct, but draws a very complicated boundary, thus overfitting the data. See the following figures for the decision boundaries you should get for different values of $\\lambda$. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">\n",
    "            No regularization (overfitting)<img src=\"Figures/decision_boundary3.png\">\n",
    "        </td>        \n",
    "        <td style=\"text-align:center\">\n",
    "            Decision boundary with regularization\n",
    "            <img src=\"Figures/decision_boundary2.png\">\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            Decision boundary with too much regularization\n",
    "            <img src=\"Figures/decision_boundary4.png\">\n",
    "        </td>        \n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "This is not a good decision boundary in the first figure: for example, it predicts that a point at $x = (−0.25, 1.5)$ is accepted $(y = 1)$, which seems to be an incorrect decision given the training set.\n",
    "With a larger $\\lambda$, you should see a plot that shows an simpler decision boundary which still separates the positives and negatives fairly well. However, if $\\lambda$ is set to too high a value, you will not get a good fit and the decision boundary will not follow the data so well, thus underfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section7\"></a>\n",
    "## 3 Scikit-learn (20pt)\n",
    "\n",
    "In this part, you need to use scikit-learn to find $\\theta$ for part 1 and 2, and compare with the ones you have found before. You should also verify the accuracy scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "      <th>Admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Exam 1     Exam 2  Admitted\n",
       "0  34.623660  78.024693         0\n",
       "1  30.286711  43.894998         0\n",
       "2  35.847409  72.902198         0\n",
       "3  60.182599  86.308552         1\n",
       "4  79.032736  75.344376         1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'C:\\Users\\Behnam\\Downloads\\Python\\Datasets\\ML_HW2\\ex2data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Exam 1', 'Exam 2', 'Admitted'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Admitted</th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>83.489163</td>\n",
       "      <td>48.380286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>42.261701</td>\n",
       "      <td>87.103851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>99.315009</td>\n",
       "      <td>68.775409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>55.340018</td>\n",
       "      <td>64.931938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>74.775893</td>\n",
       "      <td>89.529813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Admitted     Exam 1     Exam 2\n",
       "0          0  34.623660  78.024693\n",
       "1          0  30.286711  43.894998\n",
       "2          0  35.847409  72.902198\n",
       "3          1  60.182599  86.308552\n",
       "4          1  79.032736  75.344376\n",
       "..       ...        ...        ...\n",
       "95         1  83.489163  48.380286\n",
       "96         1  42.261701  87.103851\n",
       "97         1  99.315009  68.775409\n",
       "98         1  55.340018  64.931938\n",
       "99         1  74.775893  89.529813\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Admitted','Exam 1', 'Exam 2']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Admitted'\n",
    "predictors = data.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[target]\n",
    "X = data[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression().fit(X, y)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score when whole dataset is given for train and test: 0.890\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score when whole dataset is given for train and test: {clf.score(X, y):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we gave all the sample data for train and test and the accuracy is  0.89 which is simillar to what we've calculated before but it's not a good practice, we shouldn't give all data set for train and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = LogisticRegression().fit(X_train, y_train)\n",
    "clf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we give only train set to create and test set to the evaluate the model and as expected, the accuracy score has decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for seperate train and test sets: 0.84\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score for seperate train and test sets: {clf2.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.005983</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.977885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.957407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.969444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.008976    0.005983    0.925000     0.977885\n",
       "1  0.007976    0.002993    0.984615     0.957407\n",
       "2  0.008977    0.001995    0.969231     0.969444"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "\n",
    "pipe = make_pipeline( MinMaxScaler(), PolynomialFeatures(degree=6), LogisticRegression(max_iter = 1000) )\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_validate(pipe, X, y, cv=3, return_train_score=True, scoring = 'roc_auc')\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time       0.008643\n",
       "score_time     0.003657\n",
       "test_score     0.959615\n",
       "train_score    0.968245\n",
       "dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we want to add a Gridsearch to find the best parameters for 1 estimator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('polynomialfeatures',\n",
       "                                        PolynomialFeatures(degree=2,\n",
       "                                                           include_bias=True,\n",
       "                                                           interaction_only=False,\n",
       "                                                           order='C')),\n",
       "                                       ('svc',\n",
       "                                        SVC(C=1.0, break_ties=False,\n",
       "                                            cache_size=200, class_weight=None,\n",
       "                                            coef0=0.0,\n",
       "                                            decision_function_shape=...\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'polynomialfeatures__degree': [1, 2, 3],\n",
       "                         'standardscaler': [MinMaxScaler(copy=True,\n",
       "                                                         feature_range=(0, 1)),\n",
       "                                            RobustScaler(copy=True,\n",
       "                                                         quantile_range=(25.0,\n",
       "                                                                         75.0),\n",
       "                                                         with_centering=True,\n",
       "                                                         with_scaling=True)],\n",
       "                         'svc__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2= make_pipeline( StandardScaler(), PolynomialFeatures(), SVC())\n",
    "\n",
    "param_grid = { 'standardscaler': [MinMaxScaler(), RobustScaler() ],\n",
    "               'polynomialfeatures__degree' : [1,2,3],\n",
    "               'svc__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              \n",
    "             }\n",
    "\n",
    "grid2 = GridSearchCV(pipe2, param_grid, cv=3)\n",
    "grid2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('preprocessing', MinMaxScaler() ),\n",
    "                 ('preprocessing2', PolynomialFeatures()),\n",
    "                  ('classifier', SVC()) ])\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier': [SVC()],\n",
    "     'preprocessing': [StandardScaler(), RobustScaler()], \n",
    "     'preprocessing2__degree' : [2,3,4],\n",
    "     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "    {'classifier': [LogisticRegression(max_iter=1000)],\n",
    "     'preprocessing': [StandardScaler(), RobustScaler()], \n",
    "     'preprocessing2__degree' : [2,3,4],\n",
    "     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessing',\n",
       "                                        MinMaxScaler(copy=True,\n",
       "                                                     feature_range=(0, 1))),\n",
       "                                       ('preprocessing2',\n",
       "                                        PolynomialFeatures(degree=2,\n",
       "                                                           include_bias=True,\n",
       "                                                           interaction_only=False,\n",
       "                                                           order='C')),\n",
       "                                       ('classifier',\n",
       "                                        SVC(C=1.0, break_ties=False,\n",
       "                                            cache_size=200, class_weight=None,\n",
       "                                            coef0=0.0,\n",
       "                                            decision_function_shape='ovr',\n",
       "                                            deg...\n",
       "                                                            verbose=0,\n",
       "                                                            warm_start=False)],\n",
       "                          'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                          'preprocessing': [StandardScaler(copy=True,\n",
       "                                                           with_mean=True,\n",
       "                                                           with_std=True),\n",
       "                                            RobustScaler(copy=True,\n",
       "                                                         quantile_range=(25.0,\n",
       "                                                                         75.0),\n",
       "                                                         with_centering=True,\n",
       "                                                         with_scaling=True)],\n",
       "                          'preprocessing2__degree': [2, 3, 4]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__gamma</th>\n",
       "      <th>param_preprocessing</th>\n",
       "      <th>param_preprocessing2__degree</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': LogisticRegression(C=10, class_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'classifier': LogisticRegression(C=10, class_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022339</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RobustScaler(copy=True, quantile_range=(25.0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'classifier': LogisticRegression(C=10, class_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.032660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RobustScaler(copy=True, quantile_range=(25.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': LogisticRegression(C=10, class_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.032660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006583</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>RobustScaler(copy=True, quantile_range=(25.0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'classifier': SVC(C=1.0, break_ties=False, ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.053333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>RobustScaler(copy=True, quantile_range=(25.0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'classifier': SVC(C=1.0, break_ties=False, ca...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=1.0, break_ties=False, ca...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>3</td>\n",
       "      <td>{'classifier': SVC(C=1.0, break_ties=False, ca...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>RobustScaler(copy=True, quantile_range=(25.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=1.0, break_ties=False, ca...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>2</td>\n",
       "      <td>{'classifier': SVC(C=1.0, break_ties=False, ca...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "rank_test_score                                                                 \n",
       "1                     0.013564      0.001198         0.001400        0.000490   \n",
       "1                     0.016556      0.001017         0.002394        0.000799   \n",
       "3                     0.022339      0.005970         0.002993        0.001092   \n",
       "3                     0.013761      0.000745         0.001396        0.000488   \n",
       "3                     0.006583      0.001353         0.001994        0.000002   \n",
       "...                        ...           ...              ...             ...   \n",
       "132                   0.005786      0.001163         0.001797        0.000744   \n",
       "132                   0.004790      0.000743         0.001992        0.000889   \n",
       "132                   0.005386      0.001196         0.001995        0.000630   \n",
       "132                   0.005587      0.001497         0.001792        0.000401   \n",
       "132                   0.006182      0.000976         0.001796        0.000399   \n",
       "\n",
       "                                                  param_classifier  \\\n",
       "rank_test_score                                                      \n",
       "1                LogisticRegression(C=10, class_weight=None, du...   \n",
       "1                LogisticRegression(C=10, class_weight=None, du...   \n",
       "3                LogisticRegression(C=10, class_weight=None, du...   \n",
       "3                LogisticRegression(C=10, class_weight=None, du...   \n",
       "3                SVC(C=1.0, break_ties=False, cache_size=200, c...   \n",
       "...                                                            ...   \n",
       "132              SVC(C=1.0, break_ties=False, cache_size=200, c...   \n",
       "132              SVC(C=1.0, break_ties=False, cache_size=200, c...   \n",
       "132              SVC(C=1.0, break_ties=False, cache_size=200, c...   \n",
       "132              SVC(C=1.0, break_ties=False, cache_size=200, c...   \n",
       "132              SVC(C=1.0, break_ties=False, cache_size=200, c...   \n",
       "\n",
       "                param_classifier__C param_classifier__gamma  \\\n",
       "rank_test_score                                               \n",
       "1                               100                     NaN   \n",
       "1                                10                     NaN   \n",
       "3                               100                     NaN   \n",
       "3                               100                     NaN   \n",
       "3                                10                     0.1   \n",
       "...                             ...                     ...   \n",
       "132                             0.1                      10   \n",
       "132                             0.1                     100   \n",
       "132                             0.1                     100   \n",
       "132                             0.1                     100   \n",
       "132                           0.001                   0.001   \n",
       "\n",
       "                                               param_preprocessing  \\\n",
       "rank_test_score                                                      \n",
       "1                StandardScaler(copy=True, with_mean=True, with...   \n",
       "1                StandardScaler(copy=True, with_mean=True, with...   \n",
       "3                RobustScaler(copy=True, quantile_range=(25.0, ...   \n",
       "3                RobustScaler(copy=True, quantile_range=(25.0, ...   \n",
       "3                RobustScaler(copy=True, quantile_range=(25.0, ...   \n",
       "...                                                            ...   \n",
       "132              RobustScaler(copy=True, quantile_range=(25.0, ...   \n",
       "132              StandardScaler(copy=True, with_mean=True, with...   \n",
       "132              StandardScaler(copy=True, with_mean=True, with...   \n",
       "132              RobustScaler(copy=True, quantile_range=(25.0, ...   \n",
       "132              StandardScaler(copy=True, with_mean=True, with...   \n",
       "\n",
       "                param_preprocessing2__degree  \\\n",
       "rank_test_score                                \n",
       "1                                          2   \n",
       "1                                          4   \n",
       "3                                          4   \n",
       "3                                          2   \n",
       "3                                          4   \n",
       "...                                      ...   \n",
       "132                                        4   \n",
       "132                                        2   \n",
       "132                                        3   \n",
       "132                                        2   \n",
       "132                                        2   \n",
       "\n",
       "                                                            params  \\\n",
       "rank_test_score                                                      \n",
       "1                {'classifier': LogisticRegression(C=10, class_...   \n",
       "1                {'classifier': LogisticRegression(C=10, class_...   \n",
       "3                {'classifier': LogisticRegression(C=10, class_...   \n",
       "3                {'classifier': LogisticRegression(C=10, class_...   \n",
       "3                {'classifier': SVC(C=1.0, break_ties=False, ca...   \n",
       "...                                                            ...   \n",
       "132              {'classifier': SVC(C=1.0, break_ties=False, ca...   \n",
       "132              {'classifier': SVC(C=1.0, break_ties=False, ca...   \n",
       "132              {'classifier': SVC(C=1.0, break_ties=False, ca...   \n",
       "132              {'classifier': SVC(C=1.0, break_ties=False, ca...   \n",
       "132              {'classifier': SVC(C=1.0, break_ties=False, ca...   \n",
       "\n",
       "                 split0_test_score  split1_test_score  split2_test_score  \\\n",
       "rank_test_score                                                            \n",
       "1                              1.0           1.000000                1.0   \n",
       "1                              1.0           1.000000                1.0   \n",
       "3                              1.0           0.933333                1.0   \n",
       "3                              1.0           1.000000                1.0   \n",
       "3                              1.0           1.000000                1.0   \n",
       "...                            ...                ...                ...   \n",
       "132                            0.6           0.600000                0.6   \n",
       "132                            0.6           0.600000                0.6   \n",
       "132                            0.6           0.600000                0.6   \n",
       "132                            0.6           0.600000                0.6   \n",
       "132                            0.6           0.600000                0.6   \n",
       "\n",
       "                 split3_test_score  split4_test_score  mean_test_score  \\\n",
       "rank_test_score                                                          \n",
       "1                         0.933333           1.000000         0.986667   \n",
       "1                         0.933333           1.000000         0.986667   \n",
       "3                         0.933333           1.000000         0.973333   \n",
       "3                         0.933333           0.933333         0.973333   \n",
       "3                         0.866667           1.000000         0.973333   \n",
       "...                            ...                ...              ...   \n",
       "132                       0.600000           0.600000         0.600000   \n",
       "132                       0.600000           0.600000         0.600000   \n",
       "132                       0.600000           0.600000         0.600000   \n",
       "132                       0.600000           0.600000         0.600000   \n",
       "132                       0.600000           0.600000         0.600000   \n",
       "\n",
       "                 std_test_score  \n",
       "rank_test_score                  \n",
       "1                      0.026667  \n",
       "1                      0.026667  \n",
       "3                      0.032660  \n",
       "3                      0.032660  \n",
       "3                      0.053333  \n",
       "...                         ...  \n",
       "132                    0.000000  \n",
       "132                    0.000000  \n",
       "132                    0.000000  \n",
       "132                    0.000000  \n",
       "132                    0.000000  \n",
       "\n",
       "[252 rows x 17 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid = pd.DataFrame(grid.cv_results_)\n",
    "df_grid.sort_values(by=['rank_test_score']).set_index('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'classifier__C': 10,\n",
       " 'preprocessing': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'preprocessing2__degree': 4}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>15.107608</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.960714</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15.224296</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.733606</td>\n",
       "      <td>0.005985</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.992593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  test_score  train_score\n",
       "0  15.107608    0.002992    0.960714     1.000000\n",
       "1  15.224296    0.009975    1.000000     0.999074\n",
       "2  14.733606    0.005985    0.992308     0.992593"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = pd.DataFrame(cross_validate(grid, X, y, cv=3, n_jobs=-1,scoring='roc_auc', return_train_score=True))\n",
    "scores[['train_score', 'test_score']].boxplot()\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC without cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score without cross validation is: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_proba = grid.predict_proba(X_test)[:,1]\n",
    "print(f\"The AUC score without cross validation is: {roc_auc_score(y_test, y_pred_proba):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test 1</th>\n",
       "      <th>Test 2</th>\n",
       "      <th>Accepted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.68494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.69225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.50219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.46564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Test 1   Test 2  Accepted\n",
       "0  0.051267  0.69956         1\n",
       "1 -0.092742  0.68494         1\n",
       "2 -0.213710  0.69225         1\n",
       "3 -0.375000  0.50219         1\n",
       "4 -0.513250  0.46564         1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "path =  r'C:\\Users\\Behnam\\Downloads\\Python\\Datasets\\ML_HW2\\ex2data2.txt'\n",
    "data2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accepted</th>\n",
       "      <th>Test 1</th>\n",
       "      <th>Test 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.699560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.684940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.692250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.502190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.465640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.720620</td>\n",
       "      <td>0.538740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593890</td>\n",
       "      <td>0.494880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.484450</td>\n",
       "      <td>0.999270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.006336</td>\n",
       "      <td>0.999270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0.632650</td>\n",
       "      <td>-0.030612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accepted    Test 1    Test 2\n",
       "0           1  0.051267  0.699560\n",
       "1           1 -0.092742  0.684940\n",
       "2           1 -0.213710  0.692250\n",
       "3           1 -0.375000  0.502190\n",
       "4           1 -0.513250  0.465640\n",
       "..        ...       ...       ...\n",
       "113         0 -0.720620  0.538740\n",
       "114         0 -0.593890  0.494880\n",
       "115         0 -0.484450  0.999270\n",
       "116         0 -0.006336  0.999270\n",
       "117         0  0.632650 -0.030612\n",
       "\n",
       "[118 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data2[['Accepted', 'Test 1', 'Test 2']]\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Accepted'\n",
    "predictors = data2.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = data2[target]\n",
    "X2 = data2[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size = 0.25, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(6) #Creates new features upto degree of 6 out of initial features(for X)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 28)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No need to train the model before cross validation!\n",
    "\n",
    ">Because it's a Classification problem , we need to set the scoring to 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    'Logistic Regression C=0.01': LogisticRegression(C=0.01, max_iter=10000),\n",
    "    'Logistic Regression C=1': LogisticRegression(),\n",
    "    'Logistic Regression C=100': LogisticRegression(C=100, max_iter=10000),\n",
    "    'Logistic Regression L1': LogisticRegression(solver='saga', penalty = 'l1', max_iter=10000),\n",
    "    'Gradient Boosting' : GradientBoostingClassifier(n_estimators=500, learning_rate=0.01),\n",
    "    'Random Forest' : RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic Regression C=0.01: mean of accuracy scores = 0.784\n",
      "    Logistic Regression C=1: mean of accuracy scores = 0.773\n",
      "  Logistic Regression C=100: mean of accuracy scores = 0.750\n",
      "     Logistic Regression L1: mean of accuracy scores = 0.773\n",
      "          Gradient Boosting: mean of accuracy scores = 0.761\n",
      "              Random Forest: mean of accuracy scores = 0.727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "for estimator_name, estimator_object in estimators.items():\n",
    "     kfold = KFold(n_splits=4, random_state=4 , shuffle=True)\n",
    "     scores = cross_val_score(estimator_object, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "     \n",
    "     print(f'{estimator_name:>27}: ' + f'mean of accuracy scores = {scores.mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model has $l_2$ penalty  with $C=0.01$, now we train the model with all train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_final = LogisticRegression(C=0.01, max_iter=10000).fit(X_train, y_train)\n",
    "clf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for seperate test set: 0.833\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score for seperate test set: {clf_final.score(X_test, y_test) :0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final result is a model with $83$% accuracy on test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All in one pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('polynomialfeatures',\n",
       "                 PolynomialFeatures(degree=6, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('svc',\n",
       "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(),  PolynomialFeatures(6), SVC() )\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.035904</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.957692</td>\n",
       "      <td>0.980556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.035904    0.010971    0.957143     0.987500\n",
       "1  0.006983    0.003988    0.892308     0.983333\n",
       "2  0.007988    0.003980    0.957692     0.980556"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(pipe, X, y, cv=3, scoring = 'roc_auc',return_train_score=True)\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time       0.016958\n",
       "score_time     0.006313\n",
       "test_score     0.935714\n",
       "train_score    0.983796\n",
       "dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
